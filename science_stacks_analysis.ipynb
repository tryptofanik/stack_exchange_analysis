{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from glob import glob\n",
    "import networkx as nx\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn3\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setups\n",
    "\n",
    "pd.options.display.max_columns = 50\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dicts which will contain data frames for each field of knowledge\n",
    "bio = {}\n",
    "chem = {}\n",
    "ph = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all dfs\n",
    "for domain, data_dict in tqdm(zip(['biology', 'chemistry', 'physics'], [bio, chem, ph])):\n",
    "    for path in glob(f'{domain}/*.csv'):\n",
    "        print(path)\n",
    "        name = path.split('/')[-1].split('.')[0]\n",
    "        data_dict[name] = pd.read_csv(path, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sets of tags from each stack\n",
    "bio_tags = set(bio['Tags'].TagName.tolist())\n",
    "chem_tags = set(chem['Tags'].TagName.tolist())\n",
    "ph_tags = set(ph['Tags'].TagName.tolist())\n",
    "all_tags = bio_tags | chem_tags | ph_tags\n",
    "\n",
    "# see venn graph of tags \n",
    "venn3([bio_tags, chem_tags, ph_tags], set_labels=['bio', 'chem', 'physics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tags that are in all three domains\n",
    "bio_tags & chem_tags & ph_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only in bio and chem not in ph\n",
    "bio_tags & chem_tags - ph_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove '<>' from tags\n",
    "def split_tags(string):\n",
    "    if isinstance(string, str):\n",
    "        string = string.lstrip('<').rstrip('>')\n",
    "        strings = string.split('><')\n",
    "        return strings\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for base in [bio, chem, ph]:\n",
    "    base['Posts']['Tags'] = base['Posts']['Tags'].apply(split_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio['Posts']['Tags'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create graph where each node is a tag and they are conneted if they cooccur in the same post\n",
    "# the more frequent they coocur the higher the weight\n",
    "def create_graph(dbs):\n",
    "    graph = nx.Graph()\n",
    "    for db in dbs:\n",
    "        graph.add_nodes_from(db['Tags'].TagName.tolist())\n",
    "    tag_pairs_map = defaultdict(int)\n",
    "    for db in dbs:\n",
    "        for post_tags in db['Posts']['Tags'].tolist():\n",
    "            tag_pairs = itertools.combinations(post_tags, r=2)\n",
    "            for tag1, tag2 in tag_pairs:  # order\n",
    "                if tag1 > tag2:\n",
    "                    tag1, tag2 = tag2, tag1\n",
    "                tag_pairs_map[(tag1, tag2)] += 1\n",
    "\n",
    "    graph.add_weighted_edges_from(\n",
    "        [(tag1, tag2, cnt) for (tag1, tag2), cnt in tag_pairs_map.items()]\n",
    "    )\n",
    "    return graph\n",
    "    \n",
    "tag_graph = create_graph([bio, chem, ph])\n",
    "bio_graph = create_graph([bio])\n",
    "chem_graph = create_graph([chem])\n",
    "ph_graph = create_graph([ph])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_graph.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_graph.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "nx.draw_kamada_kawai(bio_graph, edge_color=(0,0,0,0.15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "nx.draw_spring(bio_graph, edge_color=(0,0,0,0.15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign color to tags from domains and their combinations\n",
    "color_assignment = {\n",
    "    (True, False, False): 1,\n",
    "    (False, True, False): 2,\n",
    "    (False, False, True): 3,\n",
    "    (True, True, False): 4,\n",
    "    (True, False, True): 5,\n",
    "    (False, True, True): 6,\n",
    "    (True, True, True): 7,\n",
    "}\n",
    "\n",
    "tag_colors = []\n",
    "for tag in tag_graph.nodes():\n",
    "    color = color_assignment[(tag in bio_tags, tag in chem_tags, tag in ph_tags)]\n",
    "    tag_colors.append(color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot graph using kamada kawai algorithm\n",
    "plt.figure(figsize=(20,20))\n",
    "nx.draw_kamada_kawai(tag_graph, node_color=tag_colors, edge_color=(0,0,0,0.15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags count and frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort tags by frequency\n",
    "bio_sorted_tags = sorted([(tag, d) for tag, d in bio_graph.degree()], reverse=True, key=lambda x: x[1])\n",
    "chem_sorted_tags = sorted([(tag, d) for tag, d in chem_graph.degree()], reverse=True, key=lambda x: x[1])\n",
    "ph_sorted_tags = sorted([(tag, d) for tag, d in ph_graph.degree()], reverse=True, key=lambda x: x[1])\n",
    "bio_sorted_tags[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw normalized distribution of tag frequency\n",
    "go.Figure(\n",
    "    [\n",
    "        go.Scatter(\n",
    "            x=list(range(len(bio_sorted_tags))),\n",
    "            y=[i[1] / max(bio_sorted_tags, key=lambda x: x[1])[1] for i in bio_sorted_tags],\n",
    "            hovertext=[i[0] for i in bio_sorted_tags],\n",
    "            name='bio', line_color='green', mode='lines'\n",
    "        ),\n",
    "        go.Scatter(\n",
    "            x=list(range(len(chem_sorted_tags))),\n",
    "            y=[i[1] / max(chem_sorted_tags, key=lambda x: x[1])[1] for i in chem_sorted_tags],\n",
    "            hovertext=[i[0] for i in chem_sorted_tags],\n",
    "            name='chem', line_color='blue', mode='lines'\n",
    "        ),\n",
    "        go.Scatter(\n",
    "            x=list(range(len(ph_sorted_tags))),\n",
    "            y=[i[1] / max(ph_sorted_tags, key=lambda x: x[1])[1] for i in ph_sorted_tags],\n",
    "            hovertext=[i[0] for i in ph_sorted_tags],\n",
    "            name='ph', line_color='red', mode='lines'\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clustering by betweeness centrality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_btw_centrality = nx.edge_betweenness_centrality(bio_graph, normalized=True, weight='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_btw_centrality = sorted(bio_btw_centrality.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_graph_clusters = bio_graph.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bio_btw_centrality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = [i[0] for i in bio_btw_centrality[:13000]]\n",
    "bio_graph_clusters.remove_edges_from(to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(nx.connected_components(bio_graph_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(nx.betweenness_centrality(bio_graph).items(), key=lambda x: x[1], reverse=True)[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(nx.betweenness_centrality(tag_graph).items(), key=lambda x: x[1], reverse=True)[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags through time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for db in [bio, chem, ph]:\n",
    "    db['Posts']['CreationDate'] = pd.to_datetime(bio['Posts']['CreationDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month = bio['Posts']['CreationDate'].dt.month\n",
    "year = bio['Posts']['CreationDate'].dt.year\n",
    "bio['Posts'].loc[:, 'y_month'] = year.astype(str) + '-' + month.astype(str).str.zfill(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags_in_time = bio['Posts'].loc[:, ['y_month', 'Tags']].explode('Tags')\n",
    "all_tags_in_time.loc[:, 'cnt'] = 1\n",
    "all_tags_in_time = all_tags_in_time.groupby(['y_month', 'Tags']).count().reset_index()\n",
    "all_tags_in_time = all_tags_in_time.sort_values(['y_month', 'Tags']).reset_index(drop=True)\n",
    "top50 = all_tags_in_time.Tags.value_counts().head(50).index.tolist()\n",
    "tags_in_time = all_tags_in_time.loc[all_tags_in_time.Tags.isin(top50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yr_month_cnt = all_tags_in_time.groupby('y_month')['cnt'].sum().to_dict()\n",
    "tags_in_time.loc[:, 'freq'] = tags_in_time.cnt / tags_in_time.y_month.map(yr_month_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(\n",
    "    tags_in_time.groupby('y_month').sum().reset_index(),\n",
    "    x='y_month',\n",
    "    y='cnt', \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "for tag, data in tags_in_time.groupby('Tags'):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=data.y_month, y=data.freq,\n",
    "            mode='lines', \n",
    "            line=dict(width=1),\n",
    "            name=tag\n",
    "        )\n",
    "    )\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    yaxis_range=[0, 0.08]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_tags = ['entomology', 'species-identification', 'human-biology', 'bioinformatics']\n",
    "fig = go.Figure()\n",
    "for tag, data in tags_in_time.loc[tags_in_time.Tags.isin(variable_tags)].groupby('Tags'):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=data.y_month, y=data.freq,\n",
    "            mode='lines', \n",
    "            line=dict(width=1),\n",
    "            name=tag\n",
    "        )\n",
    "    )\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    yaxis_range=[0, 0.08],\n",
    "    yaxis_title='frequency of tag'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a very interesting thing regarding entomology and species-identification tags. They tend to have strong periodity through time especially during the summer months. Possible explanation is that during those months there is a drastic increase in the abundance of organizms that people would like to identify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marek TODO\n",
    "\n",
    "- zbadaÄ‡ zaleznosc pomiedzy frekwencja taga a nagrodami nobla\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import STL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_iden = tags_in_time.loc[tags_in_time.Tags=='species-identification']\n",
    "stl = STL(spec_iden.freq, seasonal=13, period=12)\n",
    "res = stl.fit()\n",
    "fig = go.Figure(\n",
    "    layout=dict(title='Species-identification tag occurance decomposition over time')\n",
    ")\n",
    "fig.add_traces(\n",
    "    [\n",
    "        go.Scatter(x=spec_iden.y_month, y=res.observed, line_shape='spline', name='observed'),\n",
    "        go.Scatter(x=spec_iden.y_month, y=res.trend, line_shape='spline', name='trend'),\n",
    "        go.Scatter(x=spec_iden.y_month, y=res.seasonal, line_shape='spline', name='seasonal'),\n",
    "        go.Scatter(x=spec_iden.y_month, y=res.resid, line_shape='spline', name='residual', line_dash='dashdot'),\n",
    "        \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posts texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import corpus\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removal of html tags\n",
    "html_tag = re.compile('<.*?>')\n",
    "new_line = re.compile('\\n')\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    if isinstance(raw_html, str):\n",
    "        cleantext = re.sub(html_tag, '', raw_html)\n",
    "        cleantext = re.sub(new_line, ' ', cleantext)\n",
    "        return cleantext\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for db in [bio, chem, ph]:\n",
    "    db['Posts']['Body'] = db['Posts']['Body'].apply(cleanhtml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio['Posts']['Body'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the most important step of preprocessing\n",
    "for db in [bio, chem, ph]:\n",
    "    db['Posts']['words'] = None\n",
    "    post_words = []\n",
    "    for Id, data in db['Posts'].groupby(db['Posts'].index):\n",
    "        words = nltk.word_tokenize(data['Body'].values[0])  # tokenize\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]  # lemmatize\n",
    "        words = [word.lower() for word in words if word.isalpha()]  # to lower and remove non-words\n",
    "        words = [word for word in words if word not in stopwords]  # remove stopwords\n",
    "        post_words.append(words)\n",
    "    db['Posts'].loc[:, 'words'] = post_words\n",
    "del post_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio['Posts'].loc[:, 'domain'] = 'bio'\n",
    "chem['Posts'].loc[:, 'domain'] = 'chem'\n",
    "ph['Posts'].loc[:, 'domain'] = 'ph'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decrease the number of posts for faster calculations and concatenate dfs\n",
    "all_posts = pd.concat([bio['Posts'].sample(10000), chem['Posts'].sample(10000), ph['Posts'].sample(10000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_words = all_posts.words.apply(lambda x: ' '.join(x))\n",
    "\n",
    "# transform words into tfidf vectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words={'english'}, max_features=3000)\n",
    "X = vectorizer.fit_transform(posts_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the dimensionality\n",
    "X = PCA(50).fit_transform(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map 50D space into 2D with t-SNE\n",
    "# calculates for 50 minutes! Just load the precalculated image\n",
    "tsne = TSNE(2, perplexity=70, n_iter=3000)\n",
    "mapped = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_posts.loc[:, 'x_tsne'] = mapped[:, 0]\n",
    "all_posts.loc[:, 'y_tsne'] = mapped[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize t-SNE mapping; show only posts with tags\n",
    "mask = all_posts['Tags'].apply(lambda x: x != [])\n",
    "fig = go.Figure()\n",
    "for domain, data in all_posts.loc[mask].groupby('domain'):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=data.loc[:, 'x_tsne'], y=data.loc[:, 'y_tsne'],\n",
    "            marker=dict(size=3), mode='markers', name=domain,\n",
    "            hovertext=data.loc[:, 'Tags'].apply(lambda x: ' '.join(x))\n",
    "        )\n",
    "    )\n",
    "fig.update_layout(\n",
    "    xaxis=dict(scaleanchor='y', scaleratio=1),\n",
    "    width=1500, height=1500\n",
    ").show('browser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save calculated figure as json\n",
    "fig.write_json('tsne.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json to make figure\n",
    "plotly.io.from_json(open('tsne.json', 'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering of posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = AgglomerativeClustering(30).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_posts.loc[:, 'cluster'] = clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_posts.loc[:, 'tags_joined'] = all_posts.loc[:, 'Tags'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(\n",
    "    all_posts.loc[mask],\n",
    "    x='x_tsne', y='y_tsne',\n",
    "    color='domain', animation_frame='cluster',\n",
    "    hover_data=['tags_joined']\n",
    ").update_layout(\n",
    "    xaxis=dict(scaleanchor='y', scaleratio=1),\n",
    "    width=1000, height=1000\n",
    ").show('browser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make df containing clusters, tags and domains\n",
    "cluster_tags = dict(cluster=[], tags=[], domain=[])\n",
    "for (cluster, Id), data in all_posts.groupby(['cluster', 'Id']):\n",
    "    cluster_tags['domain'].append(data['domain'].values[0])\n",
    "    cluster_tags['tags'].append(data['Tags'].values[0])\n",
    "    cluster_tags['cluster'].append(cluster)\n",
    "cluster_tags = pd.DataFrame(cluster_tags).explode('tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_per_cluster = pd.pivot_table(cluster_tags, index='cluster', columns='tags', aggfunc='count')\\\n",
    ".fillna(0).astype(int)['domain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tags = cluster_tags.tags.value_counts().head(50).index.tolist()\n",
    "tags_per_cluster.loc[:, top_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top5 tags per each cluster\n",
    "\n",
    "def f(x):\n",
    "    return x.sort_values('domain', ascending=False).head(5).reset_index()['tags'].tolist()\n",
    "\n",
    "top5 = cluster_tags.groupby(['cluster', 'tags']).count().groupby('cluster').apply(f)\n",
    "for Id, tags in top5.iteritems():\n",
    "    print(f'{Id}) {tags}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping between words and their int ID\n",
    "dictionary = gensim.corpora.Dictionary(all_posts.words.tolist())\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate words into their ID\n",
    "corpus = [dictionary.doc2bow(text) for text in all_posts.words.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA modelling\n",
    "lda_model = gensim.models.LdaModel(\n",
    "    corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=30,\n",
    "    offset=2,\n",
    "    random_state=100,\n",
    "    update_every=1,\n",
    "    passes=10,\n",
    "    alpha='auto',\n",
    "    eta=\"auto\",\n",
    "    per_word_topics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detected topics\n",
    "for Id, formula in lda_model.print_topics():\n",
    "    print(f'{formula}', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "321.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
